# FastAPI Application

This application is a FastAPI server that uses the Llama model to answer questions. It receives a list of messages and parameters for the Llama model, then returns the model's response.

## Prerequisites

- Python 3.6 or higher
- FastAPI
- Uvicorn (for running the FastAPI server)
- Llama-cpp-python
- OpenAI
- Pydantic

## Setup

1. Clone this repository to your local machine.
2. Navigate to the project directory.

```bash
cd llama_fastapi


## Create a virtual environment

```bash
python3 -m venv env

